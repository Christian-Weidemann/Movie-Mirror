{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:31.771220800Z",
     "start_time": "2023-12-20T13:09:31.708380900Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import bipartite\n",
    "from collections import Counter\n",
    "from networkx.linalg.graphmatrix import adjacency_matrix\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Loading data pathes and I/O functions from script\n",
    "from scripts.io import load_movie_titles, load_raw_bipartite, save_projection, load_projection, save_edgelist, projection_path\n",
    "\n",
    "import scripts.recommend as rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading bipartite graph and movie titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:31.932810900Z",
     "start_time": "2023-12-20T13:09:31.719220700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie titles loaded.\n",
      "Graph loaded.\n"
     ]
    }
   ],
   "source": [
    "title_dict, node_dict = load_movie_titles(\"movie-titles.txt\")\n",
    "G = load_raw_bipartite(\"full_bipartite.p\")\n",
    "\n",
    "# Split the graph into 2 sets: user and movie nodes\n",
    "user_nodes, movie_nodes = nx.algorithms.bipartite.basic.sets(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple weights projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:32.633824100Z",
     "start_time": "2023-12-20T13:09:31.809811300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection loaded.\n",
      "Projection loaded.\n"
     ]
    }
   ],
   "source": [
    "# Projecting on users\n",
    "simple_weights_users_path = \"simple_weights_users.p\"\n",
    "\n",
    "if os.path.exists(projection_path+simple_weights_users_path):\n",
    "    simple_weights_users = load_projection(simple_weights_users_path)\n",
    "else:\n",
    "    simple_weights_users = bipartite.weighted_projected_graph(G, user_nodes, ratio=True)\n",
    "    save_projection(simple_weights_users, simple_weights_users_path)\n",
    "\n",
    "# Projecting on movies\n",
    "simple_weights_movies_path = \"simple_weights_movies.p\"\n",
    "\n",
    "if os.path.exists(projection_path+simple_weights_movies_path):\n",
    "    simple_weights_movies = load_projection(simple_weights_movies_path)\n",
    "else:\n",
    "    simple_weights_movies = bipartite.weighted_projected_graph(G, movie_nodes, ratio=True)\n",
    "    save_projection(simple_weights_movies, simple_weights_movies_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question\n",
    "How can we recommend movies, given movies you like?\n",
    "\n",
    "#### Idea for an algorithm where projections take ratings into account in some way:\n",
    "1. Get input of movies you like, M, which have ratings by users U\n",
    "2. Project U and M onto U to get U1\n",
    "3. Project U1, and all movies they rated, onto the movies to get M1\n",
    "4. Recommend, from M1, the highest weight neighbor(s) of M.\n",
    "\n",
    "#### Other ideas:\n",
    "* Sample users who liked the movie(s), project onto their rated movies, recommend highest weight neighbor of liked movies.  \n",
    "* Construct similar movies to the ones you like, use this to find users like you, and iterate to converge on movies you will like.  \n",
    "\n",
    "Potential problem: if our sample of users is too large, we are likely to just recommend the most rated movies, not specific movies you would like. To test this, we could plot correlation between movie degree and likelihood to recommend.  \n",
    "\n",
    "Does backboning M1 improve recommendations?  \n",
    "\n",
    "How can we take movie genre into account for recommendation?  \n",
    "\n",
    "Are our movie recommendations associated with genre? i.e. does M1 have high genre\n",
    "homophily?  \n",
    "\n",
    "Are our recommendations largely popular or niche movies and why?  \n",
    "\n",
    "For evaluation, can we use cross validation by comparing usersâ€™ ratings to how likely we are to recommend each rated movie, based on a sample of movies they like?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Projection Methods\n",
    "\n",
    "## Rating Allocation\n",
    "Projection algorithm inspired by resource allocation, where directed edge weight from movie1 to movie2 is computed by summing over all users who rated both movies, multiplying their rating of movie1, normalized by all movie1's ratings, with their rating of movie2, normalized by the users total ratings.\n",
    "\n",
    "$$RA_{m1,m2} = \\sum_{u \\in N_{m1} \\cap N_{m2}} \\frac{w_{m1,u}}{\\sum{w_{m1}}}\\frac{w_{u,m2}}{\\sum{w_{u}}}$$ \n",
    "\n",
    "this is computed for all $m1,m2$ pairs of movies, excluding self-loops, to produce a directed movie graph.\n",
    "\n",
    "An issue with this approach: Ratings below average (e.g. 1) increase weight compared to no rating, which seems intuitively wrong.  \n",
    "We solve this by replacing no rating with the average rating, for both $w_{m1,u}$ and $w_{u,m2}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:32.645742Z",
     "start_time": "2023-12-20T13:09:32.642827100Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_common_neighbors(u, v):\n",
    "    N_u = G.neighbors(u)\n",
    "    N_v = G.neighbors(v)\n",
    "    return set(N_u) & set(N_v)\n",
    "\n",
    "def find_neighbor_difference(u, v):\n",
    "    N_u = G.neighbors(u)\n",
    "    N_v = G.neighbors(v)\n",
    "    return set(N_u).difference(set(N_v))\n",
    "\n",
    "def find_non_neighboring_nodes(u, v):\n",
    "    N_u = G.neighbors(u)\n",
    "    N_v = G.neighbors(v)\n",
    "    return set(user_nodes).difference(set(N_u).union(set(N_v)))\n",
    "\n",
    "# Old approach (incorrectly using degree)\n",
    "def rating_allocation_edge_weight(G, u, v, degree_u):\n",
    "    N_u_v = find_common_neighbors(u,v)\n",
    "    weight_u_v = 0\n",
    "    for n in N_u_v:\n",
    "        w_u_n = G.get_edge_data(u,n)['weight']\n",
    "        w_n_v = G.get_edge_data(n,v)['weight']\n",
    "        weight_u_v += w_u_n / degree_u * w_n_v / G.degree(n)\n",
    "    return weight_u_v\n",
    "\n",
    "# The algorithm works in 3 stages:\n",
    "#   1. Iterate over the common neighbors of the movies, calculate weights.\n",
    "#   2. Iterate over the differences between the neighbor sets of the movies. Calculate the weights.\n",
    "#   3. Now, there is no need to iterate over the users that are not in either movie's neighbor set. As\n",
    "#      it will be explained later, it is a one time operation.\n",
    "def rating_allocation_edge_weight_adjusted(G, u, v):\n",
    "    N_u_v = find_common_neighbors(u,v)\n",
    "    Dif_u_v = find_neighbor_difference(u, v)\n",
    "    Dif_v_u = find_neighbor_difference(v, u)\n",
    "    Non_n_u_v = find_non_neighboring_nodes(u, v)\n",
    "\n",
    "    # The total weight of a node is calculated as the average weight of the node times the number of edges. Since\n",
    "    # the bipartite network is a clique, the number of edges are the number of user nodes.\n",
    "    u_total_weight = len(user_nodes) * G.nodes[u][\"average_weight\"]\n",
    "\n",
    "    number_of_movies = len(movie_nodes)\n",
    "    number_of_users = len(user_nodes)\n",
    "    number_of_movies_inverse = 1 / number_of_movies  # Will be important later.\n",
    "    number_of_users_inverse = 1 / number_of_users\n",
    "\n",
    "    weight_u_v = 0\n",
    "    # Case 1: The intersection of the neighbors. Need the find the weights of both the edges since they are\n",
    "    # different from the default average weight.\n",
    "    for n in N_u_v:\n",
    "        w_u_n = G.get_edge_data(u,n)['weight']\n",
    "        w_n_v = G.get_edge_data(n,v)['weight']\n",
    "        weight_u_v += (w_u_n / u_total_weight * w_n_v / (number_of_movies * G.nodes[n][\"average_weight\"]))\n",
    "\n",
    "    # Case 2a: The difference of the neighboring sets of u and v. Notice the second term in the calculation. This\n",
    "    # is a simplification of (G.nodes[n][\"average_weight\"] / (G.nodes[n][\"average_weight\"] * number_of_movies)).\n",
    "    # There is no real edge between n and v, so we assume the edge weight is the average weight for node v.\n",
    "    for n in Dif_u_v:\n",
    "        w_u_n = G.get_edge_data(u, n)['weight']\n",
    "        weight_u_v += (w_u_n / u_total_weight) * number_of_movies_inverse\n",
    "\n",
    "    # Case 2b: Similar to case 2a.\n",
    "    for n in Dif_v_u:\n",
    "        w_v_n = G.get_edge_data(n, v)['weight']\n",
    "        weight_u_v += number_of_users_inverse * (w_v_n / (number_of_movies * G.nodes[n][\"average_weight\"]))\n",
    "\n",
    "    # Now, we can do the simplification shown before for both terms, since we know the edge weights will be\n",
    "    # equal to their average weights because there is no real node between u -> n and n -> v. We do this operation\n",
    "    # for the number of users that are not shared in both movies' neighboring sets.\n",
    "    weight_u_v += len(Non_n_u_v) * (number_of_users_inverse * number_of_movies_inverse)\n",
    "    return weight_u_v\n",
    "\n",
    "def rating_allocation_projection(G, movie_nodes):\n",
    "    rating_allocation_graph = nx.DiGraph()\n",
    "    for i, u in enumerate(movie_nodes):\n",
    "        for v in movie_nodes:\n",
    "\n",
    "            # Prevent self-loops\n",
    "            if v == u:\n",
    "                continue\n",
    "\n",
    "            #Change this for testing\n",
    "            edge_weight = rating_allocation_edge_weight_adjusted(G, u, v)\n",
    "            rating_allocation_graph.add_edge(u, v, weight=edge_weight)\n",
    "        \n",
    "        # print progress\n",
    "        print(f\"{i/len(movie_nodes):.0%}\")\n",
    "        print(rating_allocation_graph[u][v])\n",
    "    return rating_allocation_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:32.732443900Z",
     "start_time": "2023-12-20T13:09:32.651743900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User amount:  943\n",
      "Movie amount:  1682\n"
     ]
    }
   ],
   "source": [
    "# Possible solution to problem:\n",
    "#   1. Compute the average weight for each node and put in a dictionary. Done\n",
    "#   2. Iterate over the user nodes and check if they have a connection with the movies.\n",
    "#   3. If they do not, add an edge with average weight.\n",
    "#   4. To normalize, we add average rating * number of pretended edges at the end\n",
    "\n",
    "print(\"User amount: \", len(user_nodes))\n",
    "print(\"Movie amount: \", len(movie_nodes))\n",
    "\n",
    "def adjust_nodes(nodes):\n",
    "    for m in nodes:\n",
    "        number_of_neighbors = sum(1 for _ in G.neighbors(m))\n",
    "\n",
    "        total_weight = 0\n",
    "        for v in G.neighbors(m):\n",
    "            total_weight += G.get_edge_data(m, v)[\"weight\"]\n",
    "        average_weight = total_weight / number_of_neighbors\n",
    "\n",
    "        G.nodes[m][\"total_weight\"] = total_weight\n",
    "        G.nodes[m][\"average_weight\"] = average_weight\n",
    "        G.nodes[m][\"number_of_neighbors\"] = number_of_neighbors\n",
    "\n",
    "adjust_nodes(user_nodes)\n",
    "adjust_nodes(movie_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Allocation with genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:32.756054100Z",
     "start_time": "2023-12-20T13:09:32.732443900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Insert the genre information into the movie nodes.\n",
    "genres = dict()\n",
    "\n",
    "# Load genre information in a dict\n",
    "def create_genre_dict(genre_dict):\n",
    "    with open(\"./data/raw/genres.txt\", 'r') as genre_info:\n",
    "        for line in genre_info:\n",
    "            fields = line.split('|')\n",
    "            genre_dict[int(fields[1])] = fields[0]\n",
    "\n",
    "create_genre_dict(genres)\n",
    "\n",
    "# Load genre information of movies into the movie nodes\n",
    "def load_genres(graph, genre_dict):\n",
    "    with open(\"./data/raw/movie-titles.txt\", 'r') as movie_genres:\n",
    "        for line in movie_genres:\n",
    "            fields = line.strip().split('|')\n",
    "            movie_node = int(fields[0])\n",
    "            if not movie_node in graph:\n",
    "                continue\n",
    "            for index in range(0,19):\n",
    "                graph.nodes[movie_node][genre_dict[index]] = int(fields[index+5])\n",
    "\n",
    "            # Also adding genres as numpy array\n",
    "            graph.nodes[movie_node]['genres'] = np.array(fields[5:24], dtype=int)\n",
    "\n",
    "load_genres(G, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:32.826634300Z",
     "start_time": "2023-12-20T13:09:32.758057400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0.1304372986874877,\n 0.5945550202553832,\n array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n array([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing genre correlations\n",
    "nodes = 1, 201\n",
    "r, p = pearsonr(G.nodes[nodes[0]]['genres'], G.nodes[nodes[1]]['genres'])\n",
    "r, p, G.nodes[nodes[0]]['genres'], G.nodes[nodes[1]]['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:32.836289400Z",
     "start_time": "2023-12-20T13:09:32.833255700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding total and average weight as node attribute\n",
    "def add_weight_attributes(G, nodes):\n",
    "\n",
    "    for node in nodes:\n",
    "        neighbors = set(G.neighbors(node))\n",
    "        number_of_neighbors = len(neighbors)\n",
    "        total_weight = 0\n",
    "\n",
    "        for v in G.neighbors(node):\n",
    "            total_weight += G.get_edge_data(node, v)[\"weight\"]\n",
    "        average_weight = total_weight / number_of_neighbors\n",
    "\n",
    "        # For each non-neighbor, add average weight to total weight\n",
    "        number_of_non_neighbors = len(G.nodes) - len(nodes) - number_of_neighbors\n",
    "        total_weight += number_of_non_neighbors * average_weight\n",
    "\n",
    "        G.nodes[node][\"total_weight\"] = total_weight\n",
    "        G.nodes[node][\"average_weight\"] = average_weight\n",
    "\n",
    "# Needs to be called on users and movies seperately\n",
    "add_weight_attributes(G, user_nodes)\n",
    "add_weight_attributes(G, movie_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:32.842917300Z",
     "start_time": "2023-12-20T13:09:32.836289400Z"
    }
   },
   "outputs": [],
   "source": [
    "def chris_rating_allocation_projection(G, movie_nodes):\n",
    "    \"\"\"\n",
    "    This works, but is slower than Efe's approach\n",
    "    \"\"\"\n",
    "    user_nodes = {node for node in G.nodes if node not in movie_nodes}\n",
    "\n",
    "    rating_allocation_graph = nx.DiGraph()\n",
    "\n",
    "    for i, m1 in enumerate(movie_nodes):\n",
    "\n",
    "        m1_neighbors = set(G.neighbors(m1))\n",
    "\n",
    "        for m2 in movie_nodes:\n",
    "            # Prevent self-loops\n",
    "            if m1 == m2:\n",
    "                continue\n",
    "\n",
    "            weight_m1_m2 = 0\n",
    "\n",
    "            for user in user_nodes:\n",
    "            \n",
    "                if user in m1_neighbors:\n",
    "                    w_m1_user = G.get_edge_data(m1,user)['weight']\n",
    "                else:\n",
    "                    w_m1_user = G.nodes[m1]['average_weight']\n",
    "\n",
    "                if user in G[m2]:\n",
    "                    w_user_m2 = G.get_edge_data(user,m2)['weight']\n",
    "                else:\n",
    "                    w_user_m2 = G.nodes[user]['average_weight']\n",
    "\n",
    "                weight_m1_m2 += (w_m1_user / G.nodes[m1]['total_weight']) * (w_user_m2 / G.nodes[user]['total_weight'])\n",
    "            rating_allocation_graph.add_edge(m1, m2, weight=weight_m1_m2)\n",
    "\n",
    "        # print progress\n",
    "        print(f\"{i/len(movie_nodes):.0%}\")\n",
    "        print(m1,m2,rating_allocation_graph[m1][m2]['weight'])\n",
    "\n",
    "    return rating_allocation_graph\n",
    "\n",
    "#chris_rating_allocation_projection(G, movie_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:32.855466900Z",
     "start_time": "2023-12-20T13:09:32.849953100Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_common_neighbors(u, v):\n",
    "    N_u = G.neighbors(u)\n",
    "    N_v = G.neighbors(v)\n",
    "    return set(N_u) & set(N_v)\n",
    "\n",
    "def find_neighbor_difference(u, v):\n",
    "    N_u = G.neighbors(u)\n",
    "    N_v = G.neighbors(v)\n",
    "    return set(N_u).difference(set(N_v))\n",
    "\n",
    "def find_non_neighboring_nodes(u, v):\n",
    "    N_u = G.neighbors(u)\n",
    "    N_v = G.neighbors(v)\n",
    "    return set(user_nodes).difference(set(N_u).union(set(N_v)))\n",
    "\n",
    "# Old approach (incorrectly using degree)\n",
    "def rating_allocation_edge_weight(G, u, v, degree_u):\n",
    "    N_u_v = find_common_neighbors(u,v)\n",
    "    weight_u_v = 0\n",
    "    for n in N_u_v:\n",
    "        w_u_n = G.get_edge_data(u,n)['weight']\n",
    "        w_n_v = G.get_edge_data(n,v)['weight']\n",
    "        weight_u_v += w_u_n / degree_u * w_n_v / G.degree(n)\n",
    "    return weight_u_v\n",
    "\n",
    "def rating_allocation_edge_weight_adjusted(G, m1, m2):\n",
    "    m1_total_weight = G.nodes[m1][\"total_weight\"]\n",
    "    weight_m1_m2 = 0\n",
    "    # Case 1: The intersection of the neighbors. \n",
    "    # Need the find the weights of both the edges since they are different from the default average weight.\n",
    "    for user in find_common_neighbors(m1,m2):\n",
    "        w_m1_user = G.get_edge_data(m1,user)['weight']\n",
    "        w_user_m2 = G.get_edge_data(user,m2)['weight']\n",
    "        weight_m1_m2 += w_m1_user / m1_total_weight * w_user_m2 / G.nodes[user][\"total_weight\"]\n",
    "\n",
    "    # Case 2a: The difference of the neighboring sets of u and v. \n",
    "    # There is no real edge between user and m2, so we assume the edge weight is the average rating devided by the total ratings of user.\n",
    "    for user in find_neighbor_difference(m1, m2):\n",
    "        w_m1_user = G.get_edge_data(m1, user)['weight']\n",
    "        weight_m1_m2 += (w_m1_user / m1_total_weight) * (G.nodes[user][\"average_weight\"] / G.nodes[user][\"total_weight\"])\n",
    "\n",
    "    # Case 2b: Similar to case 2a, but instead use average for weight from m1 to user.\n",
    "    for user in find_neighbor_difference(m2, m1):\n",
    "        w_user_m2 = G.get_edge_data(user, m2)['weight']\n",
    "        weight_m1_m2 += (G.nodes[m1][\"average_weight\"] / m1_total_weight) * (w_user_m2 / G.nodes[user][\"total_weight\"])\n",
    "    \n",
    "    # Using average weights between m1 and user, and user and m2.\n",
    "    for user in find_non_neighboring_nodes(m1, m2):\n",
    "        weight_m1_m2 += (G.nodes[m1][\"average_weight\"] / m1_total_weight) * (G.nodes[user][\"average_weight\"] / G.nodes[user][\"total_weight\"])\n",
    "\n",
    "    return weight_m1_m2\n",
    "\n",
    "def rating_allocation_projection(G, movie_nodes):\n",
    "    \"\"\"\n",
    "    New approach.\n",
    "    Uses average rating of m1 for m1->user if user didn't rate m1, \n",
    "    and avg. rating of user for user->m2 if user didn't rate m2.\n",
    "    \"\"\"\n",
    "    rating_allocation_graph = nx.DiGraph()\n",
    "    for i, m1 in enumerate(movie_nodes):\n",
    "        for m2 in movie_nodes:\n",
    "            # Prevent self-loops\n",
    "            if m2 == m1:\n",
    "                continue\n",
    "            edge_weight = rating_allocation_edge_weight_adjusted(G, m1, m2)\n",
    "            rating_allocation_graph.add_edge(m1, m2, weight=edge_weight)\n",
    "        # print progress\n",
    "        print(f\"{i/len(movie_nodes):.0%}\")\n",
    "        # print(m1,m2,rating_allocation_graph[m1][m2]['weight'])\n",
    "    return rating_allocation_graph\n",
    "\n",
    "def rating_genre_allocation_projection(G, movie_nodes, genre_weight=0):\n",
    "    \"\"\"\n",
    "    Adds optional genre_weight parameter [0,1], which is weight of genre correlation compared to rating_allocation\n",
    "    \"\"\"\n",
    "    num_movies = len(movie_nodes)\n",
    "    rating_allocation_graph = nx.DiGraph()\n",
    "    for i, m1 in enumerate(movie_nodes):\n",
    "        for m2 in movie_nodes:\n",
    "            # Prevent self-loops\n",
    "            if m2 == m1:\n",
    "                continue\n",
    "            edge_weight = rating_allocation_edge_weight_adjusted(G, m1, m2)\n",
    "\n",
    "            if genre_weight > 0:\n",
    "\n",
    "                # Get pearson correlation between movie genres\n",
    "                r, _ = pearsonr(G.nodes[m1]['genres'], G.nodes[m2]['genres'])  \n",
    "\n",
    "                # Ignore negative correlations\n",
    "                r = max(0, r)  \n",
    "\n",
    "                # New edge weight is sum with correlation, weighed by genre_weight\n",
    "                edge_weight = genre_weight * r + (1-genre_weight) * edge_weight\n",
    "\n",
    "            rating_allocation_graph.add_edge(m1, m2, weight=edge_weight)\n",
    "        # print progress\n",
    "        print(f\"{i/num_movies:.0%}\")\n",
    "        # print(m1,m2,rating_allocation_graph[m1][m2]['weight'])\n",
    "    return rating_allocation_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:34.889358500Z",
     "start_time": "2023-12-20T13:09:32.859467600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection loaded.\n"
     ]
    }
   ],
   "source": [
    "# Saving/loading rating allocation projection with genres\n",
    "rating_genre_allocation_path = \"rating_genre_allocation_movies.p\"\n",
    "\n",
    "if os.path.exists(projection_path+rating_genre_allocation_path):\n",
    "    rating_genre_allocation_movies = load_projection(rating_genre_allocation_path)\n",
    "else:\n",
    "    # Takes a VERY long time\n",
    "    rating_genre_allocation_movies = rating_genre_allocation_projection(G, movie_nodes, genre_weight=0.5)\n",
    "    save_projection(rating_genre_allocation_movies, rating_genre_allocation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:36.466434200Z",
     "start_time": "2023-12-20T13:09:34.891365200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection loaded.\n"
     ]
    }
   ],
   "source": [
    "# Saving/loading rating allocation projection on movies\n",
    "rating_allocation_movies_path = \"rating_allocation_movies.p\"\n",
    "\n",
    "if os.path.exists(projection_path+rating_allocation_movies_path):\n",
    "    rating_allocation_movies = load_projection(rating_allocation_movies_path)\n",
    "else:\n",
    "    # Takes VERY long\n",
    "    rating_allocation_movies = rating_genre_allocation_projection(G, movie_nodes)\n",
    "    save_projection(rating_allocation_movies, rating_allocation_movies_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:36.466434200Z",
     "start_time": "2023-12-20T13:09:36.464437Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving edge lists for visualization with Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:36.471435500Z",
     "start_time": "2023-12-20T13:09:36.469438400Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Getting 50 highest degree movies\n",
    "\n",
    "# TODO: Pass these to save_edgelist\n",
    "# save_edgelist(50, rating_allocation_movies, \"rating_allocation_movies_edges\", title_dict, overwrite=True)\n",
    "# save_edgelist(50, simple_weights_movies, \"simple_weights_movies_edges\", title_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:36.480786300Z",
     "start_time": "2023-12-20T13:09:36.473436Z"
    }
   },
   "outputs": [],
   "source": [
    "# liked_movie_list = ['Die Hard (1988)', 'Star Wars (1977)']\n",
    "# graph = rating_allocation_movies\n",
    "# liked_movie_node_list = [node_dict[liked_movie_title] for liked_movie_title in liked_movie_list]\n",
    "# neighbors_weights = dict()\n",
    "# for liked_movie_node in liked_movie_node_list:  # For each of the liked movies\n",
    "#     for node, neighbor, attr_dict in graph.edges(liked_movie_node, data=True):  # For each of its edges\n",
    "        \n",
    "#         # Avoid edges to liked movies\n",
    "#         if neighbor in liked_movie_node_list:\n",
    "#             continue\n",
    "        \n",
    "#         # Append weight to neighbor to dict of all weights\n",
    "#         neighbors_weights.setdefault(neighbor, []).append(attr_dict['weight'])  \n",
    "\n",
    "# # Average over all weights for each movie\n",
    "# avg_neighbors_weights = [(node, mean(weights)) for node, weights in neighbors_weights.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:36.483786800Z",
     "start_time": "2023-12-20T13:09:36.477787700Z"
    }
   },
   "outputs": [],
   "source": [
    "# # To recommend, find highest weight movie from movie you like\n",
    "\n",
    "# def k_highest_weight_neighbors(k, liked_movie_title, graph):\n",
    "    \n",
    "#     liked_movie_node = node_dict[liked_movie_title]\n",
    "#     edges = list(graph.edges(liked_movie_node, data=True))\n",
    "\n",
    "#     neighbors_weights = [(neighbor, weight['weight']) for node, neighbor, weight in edges]\n",
    "#     neighbors_weights = sorted(neighbors_weights, reverse=True, key=lambda x: x[1])\n",
    "\n",
    "#     neighbors_weights_dict = dict((node, find_neighbor_weights(edges[node])) for node in edges.keys())\n",
    "\n",
    "#     n_neighbors = [(title_dict[neighbor],weight) for neighbor, weight in neighbors_weights][:k]\n",
    "\n",
    "#     print(f\"{k} highest weight neighbors of '{liked_movie_title}':\")\n",
    "#     return n_neighbors\n",
    "\n",
    "# k_highest_weight_neighbors(10, 'Die Hard (1988)', rating_allocation_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:36.526057900Z",
     "start_time": "2023-12-20T13:09:36.484787800Z"
    }
   },
   "outputs": [],
   "source": [
    "#rating_allocation_movies.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T14:09:17.021813500Z",
     "start_time": "2023-12-20T14:09:16.995930600Z"
    }
   },
   "outputs": [],
   "source": [
    "liked_movie_list = [\"Batman Forever (1995)\"]\n",
    "# print(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T14:09:18.212907300Z",
     "start_time": "2023-12-20T14:09:18.183231500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Batman Returns (1992)', 0.5798100497880322), ('Three Musketeers, The (1993)', 0.4997857914588887), ('Cliffhanger (1993)', 0.4996075564768947), ('Rumble in the Bronx (1995)', 0.49955533854694517), ('Batman & Robin (1997)', 0.4990640798011138), ('Princess Bride, The (1987)', 0.4235724323071162), ('Men in Black (1997)', 0.4230792507523231), ('True Lies (1994)', 0.42239145169878556), ('Best Men (1997)', 0.42226412122357415), ('Evil Dead II (1987)', 0.4222279164713217), ('Batman (1989)', 0.4221651001980307), ('Hard Target (1993)', 0.42210271426732116), ('Operation Dumbo Drop (1995)', 0.42203104919718926), ('Raiders of the Lost Ark (1981)', 0.4150872277979819), ('Indiana Jones and the Last Crusade (1989)', 0.4136584370219802), ('Sting, The (1973)', 0.4136115396579264), ('Some Like It Hot (1959)', 0.41296495498982744), ('Grosse Pointe Blank (1997)', 0.41279500580062917), ('Adventures of Robin Hood, The (1938)', 0.41271518121913675), ('New York Cop (1996)', 0.4127150168965514)]\n"
     ]
    }
   ],
   "source": [
    "def get_edges(liked_movie_list,d_graph):\n",
    "    liked_movie_node_list = [node_dict[liked_movie_title] for liked_movie_title in liked_movie_list]\n",
    "    edges = dict((liked_movie_node, list(d_graph.edges(liked_movie_node, data=True))) for liked_movie_node in liked_movie_node_list)\n",
    "    return edges\n",
    "\n",
    "edges = get_edges(liked_movie_list, rating_allocation_movies)\n",
    "\n",
    "def get_average_weight_per_movie(edges):\n",
    "    node_weights = dict()\n",
    "    for node, edges_list in edges.items():\n",
    "        for edge in edges_list:\n",
    "            if edge[1] in edges.keys():\n",
    "                continue\n",
    "            if edge[1] in node_weights:\n",
    "                node_weights[edge[1]].append(edge[2]['weight'])\n",
    "            else:\n",
    "                node_weights[edge[1]] = [edge[2]['weight']]\n",
    "        average_weights = [(node,mean(weights)) for node, weights in node_weights.items()]\n",
    "    return average_weights\n",
    "\n",
    "def sort_average_weight(average_weight_edges):\n",
    "    return sorted(average_weight_edges, reverse=True, key=lambda x: x[1])\n",
    "\n",
    "def k_recommend_from_list(k, rating_allocation_movies, liked_movie_list):\n",
    "    edges = get_edges(liked_movie_list, rating_allocation_movies)\n",
    "    sorted_average_weights = sort_average_weight(get_average_weight_per_movie(edges))\n",
    "    n_neighbors = [(title_dict[neighbor],weight) for neighbor, weight in sorted_average_weights][:k]\n",
    "    return n_neighbors\n",
    "print(k_recommend_from_list(20,rating_genre_allocation_movies, liked_movie_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:36.564026300Z",
     "start_time": "2023-12-20T13:09:36.558638200Z"
    }
   },
   "outputs": [],
   "source": [
    "#k = 10\n",
    "#n_neighbors = [(title_dict[neighbor],weight) for neighbor, weight in sorted_average_weights][:k]\n",
    "#print(n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T13:09:36.573582500Z",
     "start_time": "2023-12-20T13:09:36.564026300Z"
    }
   },
   "outputs": [],
   "source": [
    "# For multiple likes movies, find highest average weight movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_genres(rating_allocation_movies, genres)\n",
    "load_genres(simple_weights_movies, genres)\n",
    "load_genres(rating_genre_allocation_movies, genres)\n",
    "\n",
    "assortativity_coef_dict = dict()\n",
    "for genre in genres.values():\n",
    "    assortativity_coef_dict[genre] = [\n",
    "       nx.assortativity.attribute_assortativity_coefficient(\n",
    "           rating_allocation_movies, attribute=genre\n",
    "       ), nx.assortativity.attribute_assortativity_coefficient(\n",
    "            simple_weights_movies, attribute=genre\n",
    "        ),\n",
    "        nx.assortativity.attribute_assortativity_coefficient(\n",
    "            rating_genre_allocation_movies, attribute=genre\n",
    "        )\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(assortativity_coef_dict)\n",
    "df.insert(0, \"Projections\", [\"Resource Allocation\",\"Simple Weights\", \"Genre Allocation\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T13:10:14.214433Z",
     "start_time": "2023-12-20T13:10:14.210887900Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
