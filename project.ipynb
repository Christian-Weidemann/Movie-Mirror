{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:27.762673200Z",
     "start_time": "2023-12-05T18:04:27.724152300Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import bipartite\n",
    "from collections import Counter\n",
    "from networkx.linalg.graphmatrix import adjacency_matrix\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from timit import timeit\n",
    "\n",
    "# Loading data pathes and I/O functions from script\n",
    "from scripts.io import load_movie_titles, load_raw_bipartite, save_projection, load_projection, save_edgelist, projection_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading bipartite graph and movie titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:27.836185300Z",
     "start_time": "2023-12-05T18:04:27.729672800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie titles loaded.\n",
      "Graph loaded.\n"
     ]
    }
   ],
   "source": [
    "title_dict, node_dict = load_movie_titles(\"movie-titles.txt\")\n",
    "G = load_raw_bipartite(\"full_bipartite.p\")\n",
    "\n",
    "# Split the graph into 2 sets: user and movie nodes\n",
    "user_nodes, movie_nodes = nx.algorithms.bipartite.basic.sets(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.1 ms ± 30.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit nx.algorithms.bipartite.basic.sets(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple weights projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:30.839664900Z",
     "start_time": "2023-12-05T18:04:27.821141800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection loaded.\n",
      "Projection loaded.\n"
     ]
    }
   ],
   "source": [
    "# Projecting on users\n",
    "simple_weights_users_path = \"simple_weights_users.p\"\n",
    "\n",
    "if os.path.exists(projection_path+simple_weights_users_path):\n",
    "    simple_weights_users = load_projection(simple_weights_users_path)\n",
    "else:\n",
    "    simple_weights_users = bipartite.weighted_projected_graph(G, user_nodes, ratio=True)\n",
    "    save_projection(simple_weights_users, simple_weights_users_path)\n",
    "\n",
    "# Projecting on movies\n",
    "simple_weights_movies_path = \"simple_weights_movies.p\"\n",
    "\n",
    "if os.path.exists(projection_path+simple_weights_movies_path):\n",
    "    simple_weights_movies = load_projection(simple_weights_movies_path)\n",
    "else:\n",
    "    simple_weights_movies = bipartite.weighted_projected_graph(G, movie_nodes, ratio=True)\n",
    "    save_projection(simple_weights_movies, simple_weights_movies_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question\n",
    "How can we recommend movies, given movies you like?\n",
    "\n",
    "#### Idea for an algorithm where projections take ratings into account in some way:\n",
    "1. Get input of movies you like, M, which have ratings by users U\n",
    "2. Project U and M onto U to get U1\n",
    "3. Project U1, and all movies they rated, onto the movies to get M1\n",
    "4. Recommend, from M1, the highest weight neighbor(s) of M.\n",
    "\n",
    "#### Other ideas:\n",
    "* Sample users who liked the movie(s), project onto their rated movies, recommend highest weight neighbor of liked movies.  \n",
    "* Construct similar movies to the ones you like, use this to find users like you, and iterate to converge on movies you will like.  \n",
    "\n",
    "Potential problem: if our sample of users is too large, we are likely to just recommend the most rated movies, not specific movies you would like. To test this, we could plot correlation between movie degree and likelihood to recommend.  \n",
    "\n",
    "Does backboning M1 improve recommendations?  \n",
    "\n",
    "How can we take movie genre into account for recommendation?  \n",
    "\n",
    "Are our movie recommendations associated with genre? i.e. does M1 have high genre\n",
    "homophily?  \n",
    "\n",
    "Are our recommendations largely popular or niche movies and why?  \n",
    "\n",
    "For evaluation, can we use cross validation by comparing users’ ratings to how likely we are to recommend each rated movie, based on a sample of movies they like?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Projection Methods\n",
    "\n",
    "## Rating Allocation\n",
    "Projection algorithm inspired by resource allocation, where directed edge weight from movie1 to movie2 is computed by summing over all users who rated both movies, multiplying their rating of movie1, normalized by all movie1's ratings, with their rating of movie2, normalized by the users total ratings.\n",
    "\n",
    "$$RA_{m1,m2} = \\sum_{u \\in N_{m1} \\cap N_{m2}} \\frac{w_{m1,u}}{\\sum{w_{m1}}}\\frac{w_{u,m2}}{\\sum{w_{u}}}$$ \n",
    "\n",
    "this is computed for all $m1,m2$ pairs of movies, excluding self-loops, to produce a directed movie graph.\n",
    "\n",
    "An issue with this approach: Ratings below average (e.g. 1) increase weight compared to no rating, which seems intuitively wrong.\n",
    "Could this be solved by assuming no rating = average rating?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Possible solution to problem:\n",
    "#   1. Compute the average weight for each node and put in a dictionary. Done\n",
    "#   2. Iterate over the user nodes and check if they have a connection with the movies.\n",
    "#   3. If they do not, add an edge with average weight.\n",
    "#   4. To normalize, we add average rating * number of pretended edges at the end\n",
    "\n",
    "print(\"User amount: \", len(user_nodes))\n",
    "print(\"Movie amount: \", len(movie_nodes))\n",
    "\n",
    "def adjust_movie_nodes():\n",
    "    print(\"Movie nodes adjusted.\")\n",
    "    print(\"\")\n",
    "    for m in movie_nodes:\n",
    "        number_of_neighbors = sum(1 for _ in G.neighbors(m))\n",
    "\n",
    "        average_weight = 0\n",
    "        for v in G.neighbors(m):\n",
    "            average_weight += G.get_edge_data(m, v)[\"weight\"]\n",
    "        average_weight /= number_of_neighbors\n",
    "\n",
    "        G.nodes[m][\"average_weight\"] = average_weight\n",
    "        G.nodes[m][\"number_of_neighbors\"] = number_of_neighbors\n",
    "        print(G.nodes[m])\n",
    "\n",
    "def adjust_user_nodes():\n",
    "    print(\"User nodes adjusted.\")\n",
    "    print(\"\")\n",
    "    for u in user_nodes:\n",
    "        number_of_neighbors = sum(1 for _ in G.neighbors(u))\n",
    "\n",
    "        average_weight = 0\n",
    "        for v in G.neighbors(u):\n",
    "            average_weight += G.get_edge_data(u, v)[\"weight\"]\n",
    "        average_weight /= number_of_neighbors\n",
    "\n",
    "        G.nodes[u][\"average_weight\"] = average_weight\n",
    "        G.nodes[u][\"number_of_neighbors\"] = number_of_neighbors\n",
    "        print(G.nodes[u])\n",
    "\n",
    "adjust_movie_nodes()\n",
    "adjust_user_nodes()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:30.936728200Z",
     "start_time": "2023-12-05T18:04:30.931575700Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_common_neighbors(u, v):\n",
    "    N_u = G.neighbors(u)\n",
    "    N_v = G.neighbors(v)\n",
    "    return set(N_u) & set(N_v)\n",
    "\n",
    "def find_neighbor_difference(u, v):\n",
    "    N_u = G.neighbors(u)\n",
    "    N_v = G.neighbors(v)\n",
    "    return set(N_u).difference(set(N_v))\n",
    "\n",
    "def find_non_neighboring_nodes(u, v):\n",
    "    N_u = G.neighbors(u)\n",
    "    N_v = G.neighbors(v)\n",
    "    return set(user_nodes).difference(set(N_u).union(set(N_v)))\n",
    "\n",
    "def rating_allocation_edge_weight(G, u, v, degree_u):\n",
    "    N_u_v = find_common_neighbors(u,v)\n",
    "    weight_u_v = 0\n",
    "    for n in N_u_v:\n",
    "        w_u_n = G.get_edge_data(u,n)['weight']\n",
    "        w_n_v = G.get_edge_data(n,v)['weight']\n",
    "        weight_u_v += w_u_n / degree_u * w_n_v / G.degree(n)\n",
    "    return weight_u_v\n",
    "\n",
    "# The algorithm works in 3 stages:\n",
    "#   1. Iterate over the common neighbors of the movies, calculate weights.\n",
    "#   2. Iterate over the differences between the neighbor sets of the movies. Calculate the weights.\n",
    "#   3. Now, there is no need to iterate over the users that are not in either movie's neighbor set. As\n",
    "#      it will be explained later, it is a one time operation.\n",
    "def rating_allocation_edge_weight_adjusted(G, u, v, user_nodes):\n",
    "    N_u_v = find_common_neighbors(u,v)\n",
    "    Dif_u_v = find_neighbor_difference(u, v)\n",
    "    Dif_v_u = find_neighbor_difference(v, u)\n",
    "    Non_n_u_v = find_non_neighboring_nodes(u, v)\n",
    "\n",
    "    # The total weight of a node is calculated as the average weight of the node times the number of edges. Since\n",
    "    # the bipartite network is a clique, the number of edges are the number of user nodes.\n",
    "    u_total_weight = len(user_nodes) * G.nodes[u][\"average_weight\"]\n",
    "\n",
    "    number_of_movies = len(movie_nodes)\n",
    "    number_of_users = len(user_nodes)\n",
    "    number_of_movies_inverse = 1 / number_of_movies  # Will be important later.\n",
    "    number_of_users_inverse = 1 / number_of_users\n",
    "\n",
    "    weight_u_v = 0\n",
    "    # Case 1: The intersection of the neighbors. Need the find the weights of both the edges since they are\n",
    "    # different from the default average weight.\n",
    "    for n in N_u_v:\n",
    "        w_u_n = G.get_edge_data(u,n)['weight']\n",
    "        w_n_v = G.get_edge_data(n,v)['weight']\n",
    "        weight_u_v += (w_u_n / u_total_weight * w_n_v / (number_of_movies * G.nodes[n][\"average_weight\"]))\n",
    "\n",
    "    # Case 2a: The difference of the neighboring sets of u and v. Notice the second term in the calculation. This\n",
    "    # is a simplification of (G.nodes[n][\"average_weight\"] / (G.nodes[n][\"average_weight\"] * number_of_movies)).\n",
    "    # There is no real edge between n and v, so we assume the edge weight is the average weight for node v.\n",
    "    for n in Dif_u_v:\n",
    "        w_u_n = G.get_edge_data(u, n)['weight']\n",
    "        weight_u_v += (w_u_n / u_total_weight) * number_of_movies_inverse\n",
    "\n",
    "    # Case 2b: Similar to case 2a.\n",
    "    for n in Dif_v_u:\n",
    "        w_v_n = G.get_edge_data(n, v)['weight']\n",
    "        weight_u_v += number_of_users_inverse * (w_v_n / (number_of_movies * G.nodes[n][\"average_weight\"]))\n",
    "\n",
    "    # Now, we can do the simplification shown before for both terms, since we know the edge weights will be\n",
    "    # equal to their average weights because there is no real node between u -> n and n -> v. We do this operation\n",
    "    # for the number of users that are not shared in both movies' neighboring sets.\n",
    "    weight_u_v += len(Non_n_u_v) * (number_of_users_inverse * number_of_movies_inverse)\n",
    "    return weight_u_v\n",
    "\n",
    "def rating_allocation_projection(G, movie_nodes):\n",
    "    rating_allocation_graph = nx.DiGraph()\n",
    "    for u in movie_nodes:\n",
    "        degree_u = G.degree(u)\n",
    "        for v in movie_nodes:\n",
    "\n",
    "            # Prevent self-loops\n",
    "            if v == u:\n",
    "                continue\n",
    "\n",
    "            #Change this for testing\n",
    "            edge_weight = rating_allocation_edge_weight_adjusted(G, u, v, user_nodes)\n",
    "            rating_allocation_graph.add_edge(u, v, weight=edge_weight)\n",
    "            \n",
    "    return rating_allocation_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:32.329169600Z",
     "start_time": "2023-12-05T18:04:30.938737700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection loaded.\n"
     ]
    }
   ],
   "source": [
    "# Saving/loading rating allocation projection on movies\n",
    "rating_allocation_movies_path = \"rating_allocation_movies.p\"\n",
    "\n",
    "if os.path.exists(projection_path+rating_allocation_movies_path):\n",
    "    rating_allocation_movies = load_projection(rating_allocation_movies_path)\n",
    "else:\n",
    "    rating_allocation_movies = rating_allocation_projection(G, movie_nodes)\n",
    "    save_projection(rating_allocation_movies, rating_allocation_movies_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving edge lists for visualization with Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:32.333958200Z",
     "start_time": "2023-12-05T18:04:32.331954500Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Getting 50 highest degree movies\n",
    "\n",
    "# TODO: Pass these to save_edgelist\n",
    "# save_edgelist(50, rating_allocation_movies, \"rating_allocation_movies_edges\", title_dict, overwrite=True)\n",
    "# save_edgelist(50, simple_weights_movies, \"simple_weights_movies_edges\", title_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:32.339695100Z",
     "start_time": "2023-12-05T18:04:32.335957700Z"
    }
   },
   "outputs": [],
   "source": [
    "# liked_movie_list = ['Die Hard (1988)', 'Star Wars (1977)']\n",
    "# graph = rating_allocation_movies\n",
    "# liked_movie_node_list = [node_dict[liked_movie_title] for liked_movie_title in liked_movie_list]\n",
    "# neighbors_weights = dict()\n",
    "# for liked_movie_node in liked_movie_node_list:  # For each of the liked movies\n",
    "#     for node, neighbor, attr_dict in graph.edges(liked_movie_node, data=True):  # For each of its edges\n",
    "        \n",
    "#         # Avoid edges to liked movies\n",
    "#         if neighbor in liked_movie_node_list:\n",
    "#             continue\n",
    "        \n",
    "#         # Append weight to neighbor to dict of all weights\n",
    "#         neighbors_weights.setdefault(neighbor, []).append(attr_dict['weight'])  \n",
    "\n",
    "# # Average over all weights for each movie\n",
    "# avg_neighbors_weights = [(node, mean(weights)) for node, weights in neighbors_weights.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:32.346949Z",
     "start_time": "2023-12-05T18:04:32.341649600Z"
    }
   },
   "outputs": [],
   "source": [
    "# # To recommend, find highest weight movie from movie you like\n",
    "\n",
    "# def k_highest_weight_neighbors(k, liked_movie_title, graph):\n",
    "    \n",
    "#     liked_movie_node = node_dict[liked_movie_title]\n",
    "#     edges = list(graph.edges(liked_movie_node, data=True))\n",
    "\n",
    "#     neighbors_weights = [(neighbor, weight['weight']) for node, neighbor, weight in edges]\n",
    "#     neighbors_weights = sorted(neighbors_weights, reverse=True, key=lambda x: x[1])\n",
    "\n",
    "#     neighbors_weights_dict = dict((node, find_neighbor_weights(edges[node])) for node in edges.keys())\n",
    "\n",
    "#     n_neighbors = [(title_dict[neighbor],weight) for neighbor, weight in neighbors_weights][:k]\n",
    "\n",
    "#     print(f\"{k} highest weight neighbors of '{liked_movie_title}':\")\n",
    "#     return n_neighbors\n",
    "\n",
    "# k_highest_weight_neighbors(10, 'Die Hard (1988)', rating_allocation_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:32.354059300Z",
     "start_time": "2023-12-05T18:04:32.347951Z"
    }
   },
   "outputs": [],
   "source": [
    "#rating_allocation_movies.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:32.382062300Z",
     "start_time": "2023-12-05T18:04:32.357062900Z"
    }
   },
   "outputs": [],
   "source": [
    "liked_movie_list = [\"Stargate (1994)\"]\n",
    "# print(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(liked_movie_list,d_graph):\n",
    "    liked_movie_node_list = [node_dict[liked_movie_title] for liked_movie_title in liked_movie_list]\n",
    "    edges = dict((liked_movie_node, list(d_graph.edges(liked_movie_node, data=True))) for liked_movie_node in liked_movie_node_list)\n",
    "    return edges\n",
    "\n",
    "edges = get_edges(liked_movie_list, rating_allocation_movies)\n",
    "\n",
    "def get_average_weight_per_movie(edges):\n",
    "    node_weights = dict()\n",
    "    for node, edges_list in edges.items():\n",
    "        for edge in edges_list:\n",
    "            if edge[1] in edges.keys():\n",
    "                continue\n",
    "            if edge[1] in node_weights:\n",
    "                node_weights[edge[1]].append(edge[2]['weight'])\n",
    "            else:\n",
    "                node_weights[edge[1]] = [edge[2]['weight']]\n",
    "        average_weights = [(node,mean(weights)) for node, weights in node_weights.items()]\n",
    "    return average_weights\n",
    "\n",
    "def sort_average_weight(average_weight_edges):\n",
    "    return sorted(average_weight_edges, reverse=True, key=lambda x: x[1])\n",
    "\n",
    "def k_recommend_from_list(k, rating_allocation_movies, liked_movie_list):\n",
    "    edges = get_edges(liked_movie_list, rating_allocation_movies)\n",
    "    sorted_average_weights = sort_average_weight(get_average_weight_per_movie(edges))\n",
    "    n_neighbors = [(title_dict[neighbor],weight) for neighbor, weight in sorted_average_weights][:k]\n",
    "    return n_neighbors\n",
    "print(k_recommend_from_list(10,rating_allocation_movies, liked_movie_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:32.544574800Z",
     "start_time": "2023-12-05T18:04:32.537573200Z"
    }
   },
   "outputs": [],
   "source": [
    "#k = 10\n",
    "#n_neighbors = [(title_dict[neighbor],weight) for neighbor, weight in sorted_average_weights][:k]\n",
    "#print(n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:32.550655700Z",
     "start_time": "2023-12-05T18:04:32.545574Z"
    }
   },
   "outputs": [],
   "source": [
    "# For multiple likes movies, find highest average weight movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T18:04:32.585650700Z",
     "start_time": "2023-12-05T18:04:32.554657Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
